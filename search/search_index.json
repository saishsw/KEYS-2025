{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":""},{"location":"#introduction","title":"Introduction","text":"<p> <p>Author: Saish Swamidurai </p> <p>Advisor: Nirav Merchant </p>"},{"location":"#about","title":"About","text":"<p>[Add details about your internship here]</p> <p>This website follows the FAIR and CARE data principles and hopes to help further open science. </p>"},{"location":"#main-project","title":"Main Project","text":"<p>https://github.com/saishsw/Converse-With-Your-Data</p>"},{"location":"#results","title":"Results","text":"<p>[Results]</p>"},{"location":"#daily-logs","title":"Daily Logs","text":"<p>link to another file called logbook.md</p>"},{"location":"acknowledgments/","title":"Acknowledgments","text":""},{"location":"acknowledgments/#acknowledgments","title":"Acknowledgments","text":"<p>[I am grateful to...]</p> <p>[This material relied upon the work of ...]</p> <p>[My Internship was funded by BIO5 Institute KEYS]</p>"},{"location":"assignment1/","title":"Assignment 1","text":""},{"location":"assignment1/#assignment-1-internship-description","title":"Assignment 1: Internship Description","text":"<p>I am Saish Swamidurai and my Principle Investigator(PI) is Dr. Nirav Merchant. I am working under him to create an AI based open-source data organization platform for researchers. My work specifically will be around creating reproducible dashboards for noisy datasets - the dashboard should be able to recognize the type of data and return the relevant statistics while also visualizing it in an aesthetic manner. The purpose of this research is to streamline data organization so that researchers around the world have a standardized platform to analyze data in a variety of ways without having to create the code themselves and use multiple disorganized platforms. After creating the dashboard and visualization process, I will also be integrating Large Language Models(LLMs) to help researchers to interact with the data by asking the dashboard to perform actions like change visualization, return specific statistics, etc.</p>"},{"location":"assignment2/","title":"Assignment 2","text":""},{"location":"assignment2/#assignment-2-introduction-to-your-research","title":"Assignment 2: Introduction to your Research","text":""},{"location":"assignment2/#purpose","title":"Purpose:","text":"<p>My research consists of writing programs that organize and visualize large sets of data into reproducible dashboards which are integrated with AI. This aligns with two main goals of my lab: first, AI integration - supercharging existing programs with AI to make them more interactive and powerful, and secondly, reproducibility - ensuring that the code can be utilized by people around the globe with minimal obstacles. </p>"},{"location":"assignment2/#prior-research","title":"Prior Research","text":"<p>Previous software on this idea like QueryChat(posit-dev, 2025) a package that allows you to interact with your data using AI and Large Language Models. Our goal is to make this more powerful, organized, and reproducible. Another example of previous work on this topic is the Arizona Voter Project - which visualizes voter data in an accessible, easy to read manner (Home | the Arizona Voter Project, 2024).</p>"},{"location":"assignment2/#need-for-study","title":"Need For Study","text":"<p>Currently, researchers don\u2019t have access to the latest, powered by the strongest AI, data organization platform, so the goal of my project will be to be able to create one that is very accessible.</p>"},{"location":"assignment2/#problem-statement","title":"Problem Statement:","text":"<p>Research Question: How can one streamline LLM-integrated personal workflows to improve the efficency of data analysis and open-science.</p> <p>Hypothesis 0: Creating reproducible LLM-integrated dashboards will increase the efficiency of data analysis and open-science.</p>"},{"location":"assignment2/#references","title":"References","text":"<p>posit-dev. (2025). GitHub - posit-dev/querychat. GitHub. https://github.com/posit-dev/querychat/?tab=readme-ov-file Home | The Arizona Voter Project. (2024). The Arizona Voter Project. https://voterproject.arizona.edu/</p>"},{"location":"assignment3/","title":"Assignment 3","text":""},{"location":"assignment3/#assignment-3-materials-and-methods","title":"Assignment 3: Materials and Methods","text":""},{"location":"assignment4/","title":"Assignment 4","text":""},{"location":"assignment4/#assignment-4-results","title":"Assignment 4: Results","text":""},{"location":"assignment5/","title":"Assignment 5 Conclusion and Discussion","text":""},{"location":"assignment6/","title":"Assignment 6 Title and Short Abstract","text":""},{"location":"assignment6/#title-here","title":"Title Here","text":""},{"location":"assignment6/#short-abstract-here","title":"Short Abstract Here","text":""},{"location":"github/","title":"Github Actions","text":""},{"location":"github/#introduction","title":"Introduction","text":"<p>GitHub Actions allows for the automation of tasks within your software development life cycle. Through GitHub actions users can automatically run their software testing scripts. </p>"},{"location":"github/#key-vocabulary","title":"Key Vocabulary","text":""},{"location":"github/#workflows","title":"Workflows","text":"<p>Workflows can be used to design, test, package, release, or deploy a project on GitHub. A workflow can be added to a repository in GitHub using the file name .github/workflows. Workflows consist of one or more jobs that are scheduled/triggered by an event. </p>"},{"location":"github/#events","title":"Events","text":"<p>Events are the activities that trigger the start of a workflow. Workflows can be triggered using pus or pull requests. </p>"},{"location":"github/#runners","title":"Runners","text":"<p>Runners can be hosted by GitHub or own on your own and is basically considered as a server that has GitHub Actions runner application installed. A runner runs one job at a time and reports the progress to GitHub. </p>"},{"location":"github/#jobs","title":"Jobs","text":"<p>A job is a sequence of instructions that run on the same runner. A workflow containing multiple jobs will perform them in parallel by default. You may also set up a pipeline to conduct jobs in a specific order. </p>"},{"location":"github/#steps","title":"Steps","text":"<p>A step is a single task that can be used to execute commands in a job. An action or a shell command can be used as a step. Each step of a task runs on the same runner, allowing all of the activities in that job to exchange data.</p>"},{"location":"github/#actions","title":"Actions","text":"<p>Actions are commands that are used to join steps to create a job. Actions can be created or found on the GitHub community. </p>"},{"location":"github/#github-education","title":"GitHub Education","text":""},{"location":"github/#steps-to-enroll","title":"Steps to Enroll","text":"<ol> <li>Go to the GitHub Education Site and enter your education status as student</li> <li>From here your school email and dated documentation of your enrollment is required</li> <li>After this is approved you have access to the GitHub education student developer pack! </li> </ol>"},{"location":"github/#whats-included-and-functionality","title":"What's Included and Functionality","text":"<p>As part of the GitHub education student developer pack and GitHub global campus students and faculty are granted access to forums, Campus TV, exclusive events, and free software and subscriptions such as Canva Pro, Microsoft Azure, and VS code. </p> <p>note: the student developer pack doesn't include access to GitHub CodeSpaces </p>"},{"location":"keysassignments/","title":"KEYS Assignments","text":"<p>Assignment 1</p> <p>Assignment 2</p> <p>Assignment 3</p> <p>Assignment 4</p> <p>Assignment 5</p> <p>Assignment 6</p> <p>Poster</p>"},{"location":"logbook/","title":"Logbook","text":""},{"location":"logbook/#training-week","title":"Training Week","text":""},{"location":"logbook/#622025","title":"6/2/2025","text":""},{"location":"logbook/#632025","title":"6/3/2025","text":""},{"location":"logbook/#642025","title":"6/4/2025","text":""},{"location":"logbook/#652025","title":"6/5/2025","text":""},{"location":"logbook/#662025","title":"6/6/2025","text":""},{"location":"logbook/#keys-internship","title":"KEYS Internship","text":""},{"location":"logbook/#692025","title":"6/9/2025","text":"<p>Today, I started with learning how to use the Discovery Environment on CyVerse - which allows me to run applications such as Visual Studio Code(VS Code), RStudio, and Jupyter Notebook. Next, I started building my Vibe coding setup by downloading Cline - an AI Autonomous Coding Agent for VS Code, and I encountered a roadblock when trying to generate a Google Gemini API to use in Cline - I didn't have permission to generate a Gemini API Key. Then set up Github Copilot on VS Code as well as turning on MCP(Model Context Protocol) integration into VS Code. I followed this with completing KEYS Assignment 1: Internship Description. Next, I started learning about the current AI Landscape, the Ethics of AI, and Prompt Engineering using Dr. Tyson Swetnam's Generative AI and Prompt Engineering Workshop. I practiced these prompt engineering skills using by writing these Prompt Engineering Examples. </p>"},{"location":"logbook/#6102025","title":"6/10/2025","text":"<p>Today, I revisted the Gemini API generation issue, and I learned that Dr. Swetnam needs to go into the system and give me permission to generate an API Key from UofA's Gemini. As Dr. Swetnam was working on that, I used CyVerse's Verde AI to generate an API Key to use in Cline. I originally  used the Deepseek-R1 model, but it was too verbose and glitchy, so I switched to the llama-4-scout model, which worked better with Cline. Next, I downloaded File System, an MCP Server, into Cline, which allows Cline to read, write, create, delete, and much more regarding my files. Then, after some testing, Dr. Swetnam fixed the permissions issue, and I was able to generate a Gemini API Key and use it to power Cline - making my setup even more powerful. I used Dr. Swetnam's MapMaking 101 course to use Cline to create a Public Health Map, however, I got timedout on using the API for exceeding its Token limit at 1.6 million tokens. So I decided to switch back to VerdeAI, which has no limit, to create the Public Health Map. After multiple attemps with both LLMs, I learned of the limitations of LLMs in the modern day, as both of them couldn't complete Step 5 consistently. After a few more attempts, Verde created an HTML file for the final output - but it was incomplete and missing data. After many attempts, I went to get help from an external ChatGPT-4o, which helped me fix my code. I pushed it to github and opened a website using Github Pages: view it at Vibe Coding Examples. I then went to D2L and completed the questions assignment for our upcoming speaker, Dr. Vignesh Subbian.</p>"},{"location":"logbook/#6112025","title":"6/11/2025","text":"<p>I started the day with our first Science Seminar - which happens each wednesday morning - where I learned about Neural Networks and Clinical work with Dr. Vignesh Subbian, then we discussed our first reflection, and moved on with various annoucements concering posters, late work, and how the program will be running. Then I joined our KEYS Crew meeting, where we discussed then to schedule our recurring meetings, how logistics for the social events will work, and when to set up individual meetings with our KEYS Crew Leader. Then, I checked into lab, and started learning about Reproducibility I: Software Environments on CyVerse Foundational Open Science Skills 2024 (FOSS) course. I learned about how Environment Managers can be a solution \"Software Dependency Hell\" by allowing your computers to create unique software installation directories to run the software in - that are isolated from your computer's PATH. Then I learned how to create and manage my own custom environment in Conda (a python Environment Manager) and run a python script that outputs the Mandelbrot set. Then, I downloaded Docker, a program to assist in the delivery of reproducible science.</p>"},{"location":"logbook/#6122025","title":"6/12/2025","text":"<p>I started by learning FOSS's Reproducibility II: Run Containers. However, to follow along, I need to access GitHub Codespaces, which requires me to signed up to GitHub Education. So I applied for GitHub education with my unofficial transcript, and it should take at least a few days for me to know whether or not my application has been accepted. In the meanwhile, I used VS Code on my local device so follow along with the lesson. Through this lesson, I learned: fundamental Docker commands, how to pull an image and run the container on my computer, and learned about ports. After this, I learned that my GitHub Education application was unable to be verified due to authentication issues, so I spent time setting up my two-factor authentication system and updating my billing information, and re-applying. Once this was done, I moved on to the next FOSS lesson Reproducibility III: Building Docker Containers. I learned that I needed access to GitHub Codespaces to do the lesson, and, thankfully, my GitHub Education application was acceppted. Even though I need to wait 72 Hours to access the benefits of GitHub Education, I was able to use Codespaces, so I continued with the lesson. In this lesson I learned how to run an example container - the one I ran was this, and I also learned how to build my own Docker images. Then I learned about CyVerse through the Remote Computing with Cyverse lesson. Then, with the help of a fellow intern, I set up filesystem and Fetch on Cyverse's discovery environment's VS Code.</p>"},{"location":"logbook/#6132025","title":"6/13/2025","text":"<p>I started today by downloading Fetch, the MCP Server mentioned on the day above, onto my local computer as well. This will allow me to use its features when I am coding on the Cyverse discovery environment and when I am coding on my local device. Next I started learning about SQL through the Databases and SQL: Software Carpentry Lesson, and to follow along with the lesson, I dowloaded DB Browser for SQLite. This lesson walked me through the common commands used in SQL by analyzing this dataset. I learned techniques such as sorting, filtering, calculating statistics, and data hygiene. At the very end of the lesson, I revied python and R - as they are languages commonly used to analyze large amounts of data. I then put these skills to practice by downloading DuckDB, however, I was unsure how to use it - so I watched a tutorial video on YouTube guiding me through the usefulness and practically of using DuckDB. After learning how to use DuckDB with python, I downloaded Quarto which allows me to create clean and organized visualization, which are needed for my project of creating interactive databases. Lastly I looked into Marimo, an open-source python notebook with SQL already integrated, as Dr. Merchant mentioned that it would be important and a helpful tool in completing my project.</p>"},{"location":"logbook/#6162025","title":"6/16/2025","text":"<p>I started the morning with finishing KEYS Assignment 2: Introduction to Your Research, and then updated it in the Github. Then I refined the title of my poster using a suggestion that Dr. Merchant gave me: \"Converse with Your Data: Reproducible LLM Powered Workflows for Creating Interactive Data Driven Dashboards\". However, I felt that this was to long to fit on my KEYS poster in a clean, organized manner, so I tweaked it to \"Converse with Your Data: Reproducible LLM-Integrated Dashboards.\" Once I updated my poster with the new title, I spent time refining and cleaning up my Github pages website. I used the \"iframe\" command to embed my London Public Health Data(See Day 2) into th  Vibe Coding Examples Page. I then started brainstorming, using various LLMs and Search Engines, the method in which I would begin building my software. I used my promp engineering skills to craft an extremely page-long detailed prompt for Gemini to be my assisstant in this project. I then started conversing with Gemini to start the initial steps of building the software. To learn how to create a program that uses DuckDB to pull the information schema from a dataframe, which will be one of the core components of analyzing a table of data, I used my Gemini Assisstant to create a program which pulls information_schema. I then went in and researched all of the syntax I didn't know and then read through it and understood the logic behind it. Then I deleted all the comments that Gemini put, and rewrote my own comments make sure I understood the code.</p>"},{"location":"logbook/#6172025","title":"6/17/2025","text":"<p>Today, I started by finishing my comments on the generated code to strengthen my understanding of how to extract information schema - this. Next I began learning how to integrate LLMs into my project by learning how to make LLMs convert prompts into SQL Querys, as that is what the chatbot in the software is going to be able to do. I started this by watching a video that Dr. Swetnam sent to us:  The Model Context Protocol (MCP), in which the creators of MCP talk about its use, how it works, and their hopes for the future. This provided me a sort of insight in how to think about LLM integration on my own work. I then talked to my Gemini Assistant and began to learn about LLM-integration. The first step was to make sure that I do not hardcode any APIs into any open-source code - as it is sensitive information; so I will be using Environment Variables to code them and store them in my local device. Then, I started working on creating an Evironment Variable for CyVerse's Verde AI API Key - as that doesn't have a token limit like the Gemini API. To leverage the fact that Verde is often Open-AI compliant, I used Python's OpenAI Library. I also used the python-dotenv library to code local .env files. I then created a .env file to store my Environment Variables for the Verde URL and API Key, then created a .gitignore(i.e. if you put a file name in this file - it doesn't get pushed to github) file in my root directory and put the .env file to ensure security of the LLM details. Then I started to learn how to use these Environment Variables to integrate the LLM into the software.</p>"},{"location":"logbook/#6182025","title":"6/18/2025","text":"<p>The first half of the day was spent participating in the KEYS Science Seminar. In the seminar, we first went over our reflections for the week prior and we talked about the projection of the KEYS for the following weeks. Then we listened to a presentation by Dr. Koshy, who talked many things, from her experience in the lab to applying to med school. Afterwards, we got a recap lesson on APA format, and had to edit and resubmit our Week 2 Assignments with the fixed citation format and with a minimum of two images. Then we learned about how to correctly do the Materials and Methods page, and looked at different examples. For the rest of the day, since I got back to working in lab at 1, I spent learning about DuckDB and integrating python with SQL. During this, using Gemini's Assistance from two days ago, I refined the code into the first rough draft of the schema extractor. Then using my Gemini Assisstant, I learned about how to use the LLM Environment Variables I created yesterday to turn human prompts into SQL querys that the LLM can run to grab information from the dataframe - essentially allowing the user to \"Converse\" their data.</p>"},{"location":"logbook/#6202025","title":"6/20/2025","text":"<p>Today, I started by working on the software that turns human prompts into SQL querys. After conversing with my Gemini Assistant, I had a fully coded version of a python file that does translates human prompts into SQL queries. I used the environment variables I created earlier in the week to use CyVerse Verde in my program. At the end, I tested it with dummy data, and it seemed to work. My continued task for this is to continue feeding human input and engineering the prompt to get the desired DuckDB SQL query. Then at 1pm, I joined the stand-up meeting with Dr. Swetnam, to whom I told my progress. He gave me suggestions on how to improve the website, which is also now on my to-do list, and told me to continue doing what im doing beacuse I am making good progress. I also asked him for some datasets to test both programs I have. I then started to refine my schema extractor by feeding it a dataset: Precision Aging Network. However, I was running into many errors, as it was only reading the dummy data, and not the CSV file, even when the CSV file is hardcoded into the program.</p>"},{"location":"logbook/#6232025","title":"6/23/2025","text":"<p>Today, I started by writing the 'Materials and Methods' page on my poster, and writing up the two paragraph assigngment explaining the process of my project. You can view it here: Assignment 3. Then I started to work on my schema extracton - now making it so that the file paths are not hardcoded, and so that the user can now input their own file. First, to make my workflow more efficient and more clean, I downloaded Rainbow CSV into VS Code, so that .csv files become easier to read. After using my Gemini Assistant to code the ability for the user to input files in the same directory. I tested it with the precision Aging Network, but it was always running index errors. This was because the first few lines of the file didn't have full sets of data, so I added another user question that asked: at what line in the file does the data start. This eliminated the index out of bounds error and made sure the code was running smoothly.</p>"},{"location":"logbook/#6242025","title":"6/24/2025","text":"<p>Since, by now, I had two completed functions: a schema extractor and a function that turns human prompt into SQL. Today, I worked on creating a main.py file where I import all the functions and run the funcitons in the order I want them too. This is essentially the backbone of the project: where all the different files come together. In the main.py file, I put all the user questions; some of them include \"Converse with your data: what is your inquiry?:\". This customizes all the prompts to the user in context of the finished project. Then in that file, once the user gives us the file and a natural language query, I call the method from the LLM_to_SQL file and then run the output of that(which is an SQL Query) to get the desired information from data. So now, I have a rough path working that can 1] ingest a file 2] take user input 3] use a LLM to take user input and turn it into SQL and 4] return the desired information by running the SQL.</p>"},{"location":"logbook/#6252025","title":"6/25/2025","text":"<p>Today, I started brainstomring about the full-stack project, i.e. how I want the user to interact with the software and the different features it can do. I settled on a general structure where the user inputs a .csv file of data and the software will do two things: 1] suggest the different types of visualizations based on the dataset and 2] takes input from the user on what visualizations they want and then plot them. To start creating the full-stack application by using FastAPI to code the backend of the project. I followed Gemini's code and guide and started creating the FastAPI Application in main.py, however I noticed that there was some issues in Gemini's code. Gemini wanted to put new methods in the main file for visualization, but I decided to move it to another file called visualation.py. But before I started coding, I realized that I needed to create a method that Gemini didn't include that was able to read user input from natural language and match it to a supported plot. I used Verde API to create a prompt that does that.</p>"},{"location":"logbook/#6262025","title":"6/26/2025","text":"<p>Today, I started by finished the method: recognize_visualization_request, which I changed to return a list of strings so that the final dashboard will have multiple visualizaiton. Although, I limited it to four so that the dashboard doesnt get too cluttered. I also started coding another method called suggest_visualizations which also returns a list of strings of a max length of four, but this one using Verde API to return suggestions simply based on the characterisitics of the data in the csv file. After refining both prompts for recognize_visualization_request and suggest_visualations, I finished both methods and started working on the bulk of the visualization by creating the visualization method itself. I used Gemini to create the plot logic for me for each of the different plots. And I finalized the plots that I want the dashboard to create: \"SUPPORTED_PLOT_TYPES = [\"BAR_CHART\", \"LINE_CHART\", \"SCATTER_PLOT\", \"HISTOGRAM\", \"BOX_PLOT\", \"PIE_CHART\"]\"</p>"},{"location":"logbook/#6272025","title":"6/27/2025","text":"<p>I spent the entirety of today finishing up the visualization file. I started by switching my visualization library to plotly, which is better integrated with Quarto, which is the software I will be using to create the interactive visualizations. I then used Gemini to help me finish all the code for visualization, and decided to test it with AI generated dummy data. However I was running into an error that my API Key wasn't being read, and after some time debugging, I realized it was simply because there was a typo in my API Key environment variable. Next I faced another error which was that the methods were not outputting a readable JSON file, so after some more debugging and help from Gemini, I used the rint(pio.to_json(go.Figure(generated_figures[0]), pretty=True)) function to use plotly to directly change the generated figure to a .JSON file, which worked. The dummy data outputted a bar chart, so I knew my code was working.</p>"},{"location":"logbook/#6302025","title":"6/30/2025","text":"<p>Today, I started by cleaning up this GitHub Pages website by backlogging the past four days. Now, I had a working backend that could produce graphs, so I started to finish the backedn portion of the project by creating an API using FastAPI. I created a few endpoints: ingestion, query, and visualization. After working with Gemini for a while, I had first iteration of the code for the endpoints. I tested to see if they were working by using my computer as a local host and going to its /docs url, and I could see the API working.</p>"},{"location":"logbook/#712025","title":"7/1/2025","text":"<p>Today, I continued refining my FastAPI endpoints. I learned how to test them by putting a .csv file into the API interface when going to the /docs url. The .csv I've been using is the one from pandb, which I mentioned above at: Precision Aging Network. So, I ran through all the data, and for the most part it was ingesting properly. I realized that the software was returning suggestions at the wrong endpoint, so I went through and refined the code so that it would return suggesstions at the same time as the ingestion phase, so that the user gets suggested visualizations right when the software gets their data. I also got on a quick meeting with Dr. Merchant, where I showed him my project and talked about the future trajectory for the project.</p>"},{"location":"logbook/#722025","title":"7/2/2025","text":"<p>Next, I started testing the query endpoint to make sure that the user can \"Converse with their data\" properly. After some tests, I noticed the limitations of the prompt - it would sometimes return non-SQL output. I refined the prompt that was running through the method by being extremely specific with the guidlines of the prompt. By emphasizing that the LLM should ONLY output SQL, I tested the method a few more times and it was working. I cleaned up the main.py file so that it also ran the SQL against the databsae in the same step. Lastly, I tested it and it was all working, but I also wanted the output to include the SQL itself so the user knows what the system is running, so I included it in the output. Now the second endpoint returns the SQL and the output of the SQL running.</p>"},{"location":"logbook/#732025","title":"7/3/2025","text":"<p>Today was the most difficult. I started working on the visualizations endpoint, and I just hit error after error.</p>"},{"location":"logbook/#742025","title":"7/4/2025","text":""},{"location":"poster/","title":"[Project Name Here]","text":""},{"location":"poster/#description","title":"Description:","text":"<p>[Description of your Project Here]</p>"},{"location":"poster/#introduction","title":"Introduction","text":"<p>[Introductino to your project here]</p>"},{"location":"poster/#methods","title":"Methods","text":"<p>[Methods for your project here]</p>"},{"location":"poster/#results","title":"Results","text":"<p>[Results for your project here]</p>"},{"location":"promptengineeringexamples/","title":"Prompt Engineering Examples","text":"<p>Note: These examples are using Google Gemini 2.5 Flash</p>"},{"location":"promptengineeringexamples/#example-1-keys-internship-initial-steps","title":"Example 1: KEYS Internship Initial Steps","text":"<p>System Instructions:</p> <p>You are an extremely intelligent computer scientist assistant. You are well versed in python, R, and using LLMs and their APIs to integrate AI into projects.  Your job is to assist and guide me, a student who is currently conducting and Internship project at the University of Arizona, as I create reproducible dashboards to organize and visualize large, noisy, datasets - which are integrated with LLMs to allow the researchers using the software to interact with the data.</p> <p>Make sure to: 1. Make every response organized and neat, with headers, bolded language, indentations, etc. 2. Use a professional and friendly tone when answering questions. 3. Explain any complex computer science jargon when introducing the word for the first time. 4. Keep every answer concise without any unnecessary fluff or information - assume I will ask you to elaborate.</p> <p>If any question is not as clear as you would need to provide a clear, concise, and smart solution - ask follow up questions to clarify the portions &gt; of the prompt that you do not understand.</p> <p>User Prompt:</p> <p>what is the first step I should take - explain concisely, number your points.</p> <p>Response:</p> <p>The first step you should take is Project Scoping and Requirements Definition.</p> <p>This involves:</p> <ol> <li>Clearly defining the dashboard's objectives and how researchers will use it.</li> <li>Identifying the specific types of \"noise\" in your data.</li> <li>Understanding the data sources, volume, and update frequency.</li> <li>Envisioning how LLMs will enhance data interaction. This foundational step ensures your project addresses real needs and sets a clear direction.</li> </ol> <p>User Follow-up:</p> <p>In a short list, what types of noise can be found in data?</p> <p>Response:</p> <p>Here's a numbered short list of common types of noise found in data: 1. Missing Values: Absence of data where it's expected. 2. Outliers: Data points significantly different from the rest. 3. Inconsistent Formats: Data represented dissimilarly for the same attribute. 4. Measurement Errors: Inaccuracies from data collection. 5. Duplicate Records: Redundant entries for the same entity. 6. Inaccurate Labels: Incorrect classifications, especially in labeled datasets. 7. Typographical Errors: Simple data entry mistakes. 8. Irrelevant Features: Attributes that don't add value to analysis. 9. Inconsistent Units: Values using different measurement units. 10. Stale Data: Outdated or no longer relevant information.</p>"},{"location":"promptengineeringexamples/#example-2-poker-assistant","title":"Example 2: Poker Assistant","text":"<p>System Instructions:</p> <p>You are a high-level poker assistant who helps players make the correct play at all stages of a hand, and you are a seasoned poker expert with the ability to calculate the best estimate of expected value(ev), number of outs, and what those outs are.</p> <p>I am at the final table of a high-stakes poker tournament. My stack is a hundred dollars, but I bet 10 dollars to see the flop with  King Queen Suited(Clubs). The flop turns over as Jack of diamond, 10 of hearts, and 2 of clubs. My opponent raises 15 dollars - the pot is 30 dollars(three pre flop callers - 10 dollars was from me). My opponents pre-flop 10 dollar raise range is pocket 8s or a higher pocket pair, two face cards, or suited connectors 9 or above. What should I do? </p> <p>In your answer include  1. My estimated EV 2. My number of outs 3. What those outs are, in a numbered list. 4. If I should reraise, call, fold</p> <p>Make your answers extremely concise, and ask for clarification on ambiguous information.</p> <p>System Follow-up:</p> <p>In poker, the phrase \"raises 15 dollars\" can be ambiguous. It could mean:</p> <ol> <li>Your opponent added $15 to the pot, making their total bet for the current street $15.</li> <li>Your opponent raised the previous bet by $15, meaning their total bet is the amount of the previous bet plus $15. Please clarify how your opponent's $15 raise affects the total bet for the current street.</li> </ol> <p>User Clarification:</p> <p>The pot was 30. My opponent made the pot 45. Added 15.</p> <p>Response:</p> <p>Here's the breakdown of your situation:</p> <p>Number of Outs: 13</p> <p>What those Outs are:</p> <ol> <li>Ace of Clubs</li> <li>Ace of Diamonds</li> <li>Ace of Hearts</li> <li>Ace of Spades</li> <li>9 of Clubs</li> <li>9 of Diamonds</li> <li>9 of Hearts</li> <li>9 of Spades</li> <li>2 of Clubs (for flush)</li> <li>3 of Clubs (for flush)</li> <li>4 of Clubs (for flush)</li> <li>5 of Clubs (for flush)</li> <li>6 of Clubs (for flush) Estimated EV:</li> </ol> <p>To calculate the estimated EV, we need to make some assumptions about your opponent's range and your implied odds. Given your opponent's pre-flop raising range (pocket 8s+, two face cards, or suited connectors 9+) and the board, a reasonable assumption is that your opponent likely has:</p> <ul> <li>A made straight (Q9, KQ, AJ, QJ)</li> <li>A set (JJ, TT, 22)</li> <li>Two pair (KJ, KT, J2, T2)</li> <li>Overpair (AA, KK, QQ)</li> <li>A strong draw (e.g., Axs for a flush, or a higher straight draw like AQ if they have Acs)</li> </ul> <p>Let's assume the following for a simplified EV calculation:</p> <ul> <li>Pot after your call: $30 (initial pot) + $15 (opponent's bet) + $15 (your call) = $60</li> <li>Your investment to call: $15</li> <li>Implied Odds: With your stack being $100 and the pot growing, you have decent implied odds if you hit. Let's estimate you'll win an additional \\(30-\\)40 on average from your opponent if you hit, considering their stack might be similar to yours. So, a total win of approximately \\(90-\\)100 if you hit.</li> <li>Probability of hitting an out by the river (approximately): With 13 outs, you have approximately a 26% chance of hitting by the river (using the 4/2 rule: 13 outs * 4 = 52% to hit by the river, but this is an overestimate as it doesn't account for drawing dead or hitting on the turn). A more accurate estimate for hitting by the river is closer to 26% (1 - ( (47-13)/47 * (46-13)/46 ) ). Let's use 26% for a rough estimate.</li> <li>Probability of your opponent having a hand you beat if you hit: If you hit one of your outs, you'll likely have a straight or a flush. Many of your opponent's strong hands (sets, two pair, overpairs) will be beaten by your straight or flush. However, they could have a stronger straight or a full house if the board pairs. Let's assume you win 70% of the time you hit an out that completes your hand.</li> </ul> <p>Estimated EV Calculation:</p> <p>EV = (Probability of Winning) * (Amount Won) - (Probability of Losing) * (Amount Lost)</p> <ul> <li>If you hit (approx. 26%): You win approximately $90 (pot + implied odds).</li> <li>If you miss (approx. 74%): You lose $15. EV = (0.26 * $90) - (0.74 * $15) EV = $23.40 - $11.10 EV = $12.30</li> </ul> <p>This positive EV indicates that calling is profitable in the long run.</p> <p>What you should do:</p> <p>Call</p>"},{"location":"schemapractice/","title":"Extracting Schema Commenting Practice","text":"<p>Note: The code was generated by Gemini 2.5 Flash; I wrote the comments to strenghten my understanding of the code. The lines following \"#\" are the comments.</p> <pre><code>import pandas as pd\nimport duckdb\nimport datetime\n\n# Creating a function that generates a DataFrame with test data - in the final project this will replaced with reading a file of data\ndef generate_test_data():\n    data = {\n        'product_id': [101, 102, 103, 104, 105],\n        'product_name': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam'],\n        'price': [1200.50, 25.99, 75.00, 300.75, 49.99],\n        'in_stock': [True, True, False, True, False],\n        'region': ['North', 'South', 'East', 'West', 'North'],\n        'order_date': [\n            datetime.date(2023, 1, 15),\n            datetime.date(2023, 1, 16),\n            datetime.date(2023, 2, 1),\n            datetime.date(2023, 2, 5),\n            datetime.date(2023, 3, 10)\n        ]\n    }\n    # Create a DataFrame using the defined data above\n    df = pd.DataFrame(data)\n    # Returns the top 5 rows of the DataFrame\n    print(df.head())\n    # Returns the DataFrame - all in the same function\n    return df\n# Creates a new function that transfer the DataFrame to DuckDB\ndef transfer_data_to_duckdb(df, table_name='my_test_data'):\n    # Makes an connection object that connects to an in-memory database - i.e it uses RAM\n    con_object = duckdb.connect(database=':memory:')\n    # This directly transfers the DataFrame to DuckDB, a neat feature of DuckDB and quick data integration with python.  \n    con_object.register(table_name, df)\n    # Prints the number of rows inserted into the table - SELECT COUNT - SQL Query\n    print(f\"Data loaded into DuckDB. Rows inserted: {con_object.execute(f'SELECT COUNT(*) FROM {table_name}').fetchone()[0]}\")\n    # This function returns the connection object\n    return con_object\n\n# Function to get the schema of the DuckDB table\ndef get_schema(con, table_name):\n    # This is a SQL query via DuckDB to get column names and data types in our specific table\n    schema_query = f\"\"\"\n    SELECT\n        column_name,\n        data_type\n    FROM    \n        information_schema.columns\n    WHERE\n        table_name = '{table_name}'\n    \"\"\"\n    # This command actually executes the SQL query and fetches all results with the \"fetchall()\" method\n    schema_results = con.execute(schema_query).fetchall()\n    # This creates a dictionary as schema_dict as the key and the table name and columns as values\n    schema_dict = {\n        \"table_name\": table_name,\n        \"columns\": []\n }\n    # This loops through the results and appends each column name and data type to the schema_dict\n    for col_name, data_type in schema_results:\n        schema_dict[\"columns\"].append({\n            \"column_name\": col_name,\n            \"data_type\": data_type\n        })\n    return schema_dict\n\n# This ensures that the code is reproducible as it runs only when it is executired directly and not an imported module - as only when it is             executed directly does __name__ = \"__main__\"\nif __name__ == \"__main__\":\n\n    dummy_df = generate_test_data()\n\n\n    duckdb_conn = transfer_data_to_duckdb(dummy_df, table_name=\"product_sales\")\n\n\n    extracted_schema = get_schema(duckdb_conn, \"product_sales\")\n\n    # Prints the extracted schema\n    print(\"\\n--- Extracted Schema ---\")\n    print(f\"Table Name: {extracted_schema['table_name']}\")\n    print(\"Columns:\")\n    for col in extracted_schema['columns']:\n        # Prints each column and its duckdb data type\n        print(f\"  - {col['column_name']} (DuckDB Type: {col['data_type']})\")\n\n    # Closes the DuckDB in memory connection\n    duckdb_conn.close()\n    print(\"\\nDuckDB connection closed.\")\n</code></pre>"},{"location":"vibecoding/","title":"Vibe Coding Example Creation","text":"<p>Note: This was created using the instructions in Dr. Swetnam's MapMaking 101 course. Here is the link to the website proper.</p>"},{"location":"vibecoding/#london-public-health-data-map","title":"London Public Health Data Map","text":""}]}