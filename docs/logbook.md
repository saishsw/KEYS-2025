# Logbook


## Training Week


### 6/2/2025
### 6/3/2025
### 6/4/2025
### 6/5/2025
### 6/6/2025

## KEYS Internship

### 6/9/2025
Today, I started with learning how to use the Discovery Environment on CyVerse - which allows me to run applications such as Visual Studio Code(VS Code), RStudio, and Jupyter Notebook. Next, I started building my Vibe coding setup by downloading Cline - an AI Autonomous Coding Agent for VS Code, and I encountered a roadblock when trying to generate a Google Gemini API to use in Cline - I didn't have permission to generate a Gemini API Key. Then set up Github Copilot on VS Code as well as turning on MCP(Model Context Protocol) integration into VS Code. I followed this with completing [KEYS Assignment 1: Internship Description](assignment1.md). Next, I started learning about the current AI Landscape, the Ethics of AI, and Prompt Engineering using Dr. Tyson Swetnam's [Generative AI and Prompt Engineering Workshop](https://tyson-swetnam.github.io/intro-gpt/). I practiced these prompt engineering skills using by writing these [Prompt Engineering Examples](promptengineeringexamples.md). 
### 6/10/2025
Today, I revisted the Gemini API generation issue, and I learned that Dr. Swetnam needs to go into the system and give me permission to generate an API Key from UofA's Gemini. As Dr. Swetnam was working on that, I used [CyVerse's Verde AI](https://chat.cyverse.ai/) to generate an API Key to use in Cline. I originally  used the Deepseek-R1 model, but it was too verbose and glitchy, so I switched to the llama-4-scout model, which worked better with Cline. Next, I downloaded [File System](https://github.com/modelcontextprotocol/servers/tree/main/src/filesystem), an MCP Server, into Cline, which allows Cline to read, write, create, delete, and much more regarding my files. Then, after some testing, Dr. Swetnam fixed the permissions issue, and I was able to generate a Gemini API Key and use it to power Cline - making my setup even more powerful. I used Dr. Swetnam's [MapMaking 101](https://tyson-swetnam.github.io/intro-gpt/tutorials/publichealth/gis/) course to use Cline to create a Public Health Map, however, I got timedout on using the API for exceeding its Token limit at 1.6 million tokens. So I decided to switch back to VerdeAI, which has no limit, to create the Public Health Map. After multiple attemps with both LLMs, I learned of the limitations of LLMs in the modern day, as both of them couldn't complete [Step 5](https://tyson-swetnam.github.io/intro-gpt/tutorials/publichealth/gis/#step-5-build-a-storytelling-leaflet-map) consistently. After a few more attempts, Verde created an HTML file for the final output - but it was incomplete and missing data. After many attempts, I went to get help from an external [ChatGPT-4o](https://openai.com/index/hello-gpt-4o/), which helped me fix my code. I pushed it to github and opened a website using Github Pages: view it at [Vibe Coding Examples](vibecoding.md). I then went to D2L and completed the questions assignment for our upcoming speaker, Dr. Vignesh Subbian.
### 6/11/2025
I started the day with our first Science Seminar - which happens each wednesday morning - where I learned about Neural Networks and Clinical work with Dr. Vignesh Subbian, then we discussed our first reflection, and moved on with various annoucements concering posters, late work, and how the program will be running. Then I joined our KEYS Crew meeting, where we discussed then to schedule our recurring meetings, how logistics for the social events will work, and when to set up individual meetings with our KEYS Crew Leader. Then, I checked into lab, and started learning about [Reproducibility I: Software Environments](https://foss.cyverse.org/06_reproducibility_I/) on [CyVerse Foundational Open Science Skills 2024](https://foss.cyverse.org/) (FOSS) course. I learned about how Environment Managers can be a solution "Software Dependency Hell" by allowing your computers to create unique software installation directories to run the software in - that are isolated from your computer's PATH. Then I learned how to create and manage my own custom environment in [Conda](https://docs.conda.io/projects/conda/en/stable/user-guide/getting-started.html) (a python Environment Manager) and run a python script that outputs the Mandelbrot set. Then, I downloaded [Docker](https://hub.docker.com/), a program to assist in the delivery of reproducible science.
### 6/12/2025
I started by learning FOSS's [Reproducibility II: Run Containers](https://foss.cyverse.org/07_reproducibility_II/). However, to follow along, I need to access [GitHub Codespaces](https://github.com/education), which requires me to signed up to [GitHub Education](https://github.com/education). So I applied for GitHub education with my unofficial transcript, and it should take at least a few days for me to know whether or not my application has been accepted. In the meanwhile, I used VS Code on my local device so follow along with the lesson. Through this lesson, I learned: fundamental Docker commands, how to pull an image and run the container on my computer, and learned about ports. After this, I learned that my GitHub Education application was unable to be verified due to authentication issues, so I spent time setting up my two-factor authentication system and updating my billing information, and re-applying. Once this was done, I moved on to the next FOSS lesson [Reproducibility III: Building Docker Containers](https://foss.cyverse.org/08_reproducibility_III/). I learned that I needed access to GitHub Codespaces to do the lesson, and, thankfully, my GitHub Education application was acceppted. Even though I need to wait 72 Hours to access the benefits of GitHub Education, I was able to use Codespaces, so I continued with the lesson. In this lesson I learned how to run an example container - the one I ran was [this](https://github.com/jeffgillan/r-script-containerized), and I also learned how to build my own Docker images. Then I learned about CyVerse through the [Remote Computing with Cyverse](https://foss.cyverse.org/09_remote_computing_cyverse/) lesson. Then, with the help of a fellow intern, I set up filesystem and [Fetch](https://github.com/modelcontextprotocol/servers/tree/main/src/fetch) on Cyverse's discovery environment's VS Code.
### 6/13/2025
I started today by downloading Fetch, the MCP Server mentioned on the day above, onto my local computer as well. This will allow me to use its features when I am coding on the Cyverse discovery environment and when I am coding on my local device. Next I started learning about [SQL](https://en.wikipedia.org/wiki/SQL) through the [Databases and SQL: Software Carpentry Lesson](https://swcarpentry.github.io/sql-novice-survey/index.html), and to follow along with the lesson, I dowloaded [DB Browser for SQLite](https://sqlitebrowser.org/about/). This lesson walked me through the common commands used in SQL by analyzing [this dataset](https://swcarpentry.github.io/sql-novice-survey/files/survey.db). I learned techniques such as sorting, filtering, calculating statistics, and data hygiene. At the very end of the lesson, I revied python and R - as they are languages commonly used to analyze large amounts of data. I then put these skills to practice by downloading [DuckDB](https://duckdb.org/), however, I was unsure how to use it - so I watched a tutorial video on YouTube guiding me through the usefulness and practically of using DuckDB. After learning how to use DuckDB with python, I downloaded [Quarto](https://quarto.org/) which allows me to create clean and organized visualization, which are needed for my project of creating interactive databases. Lastly I looked into [Marimo](https://marimo.io/), an open-source python notebook with SQL already integrated, as Dr. Merchant mentioned that it would be important and a helpful tool in completing my project.
### 6/16/2025
I started the morning with finishing [KEYS Assignment 2: Introduction to Your Research](assignment2.md), and then updated it in the Github. Then I refined the title of my poster using a suggestion that Dr. Merchant gave me: "Converse with Your Data: Reproducible LLM Powered Workflows for Creating Interactive Data Driven Dashboards". However, I felt that this was to long to fit on my KEYS poster in a clean, organized manner, so I tweaked it to "Converse with Your Data: Reproducible LLM-Integrated Dashboards." Once I updated my poster with the new title, I spent time refining and cleaning up my Github pages website. I used the "iframe" command to embed my London Public Health Data(See Day 2) into th  [Vibe Coding Examples Page](vibecoding.md). I then started brainstorming, using various LLMs and Search Engines, the method in which I would begin building my software. I used my promp engineering skills to craft an extremely page-long detailed prompt for Gemini to be my assisstant in this project. I then started conversing with Gemini to start the initial steps of building the software. To learn how to create a program that uses DuckDB to pull the information schema from a dataframe, which will be one of the core components of analyzing a table of data, I used my Gemini Assisstant to create a program which pulls information_schema. I then went in and researched all of the syntax I didn't know and then read through it and understood the logic behind it. Then I deleted all the comments that Gemini put, and rewrote my own comments make sure I understood the code.
### 6/17/2025
Today, I started by finishing my comments on the generated code to strengthen my understanding of how to extract information schema - [this](schemapractice.md). Next I began learning how to integrate LLMs into my project by learning how to make LLMs convert prompts into SQL Querys, as that is what the chatbot in the software is going to be able to do. I started this by watching a video that Dr. Swetnam sent to us: [
The Model Context Protocol (MCP)](https://www.youtube.com/watch?v=CQywdSdi5iA), in which the creators of MCP talk about its use, how it works, and their hopes for the future. This provided me a sort of insight in how to think about LLM integration on my own work. I then talked to my Gemini Assistant and began to learn about LLM-integration. The first step was to make sure that I do not hardcode any APIs into any open-source code - as it is sensitive information; so I will be using [Environment Variables](https://en.wikipedia.org/wiki/Environment_variable#:~:text=An%20environment%20variable%20is%20a,in%20which%20a%20process%20runs.) to code them and store them in my local device. Then, I started working on creating an Evironment Variable for CyVerse's Verde AI API Key - as that doesn't have a token limit like the Gemini API. To leverage the fact that Verde is often Open-AI compliant, I used Python's OpenAI Library. I also used the [python-dotenv](https://pypi.org/project/python-dotenv/) library to code local .env files. I then created a .env file to store my Environment Variables for the Verde URL and API Key, then created a .gitignore(i.e. if you put a file name in this file - it doesn't get pushed to github) file in my root directory and put the .env file to ensure security of the LLM details. Then I started to learn how to use these Environment Variables to integrate the LLM into the software.
### 6/18/2025
The first half of the day was spent participating in the KEYS Science Seminar. In the seminar, we first went over our reflections for the week prior and we talked about the projection of the KEYS for the following weeks. Then we listened to a presentation by Dr. Koshy, who talked many things, from her experience in the lab to applying to med school. Afterwards, we got a recap lesson on APA format, and had to edit and resubmit our Week 2 Assignments with the fixed citation format and with a minimum of two images. Then we learned about how to correctly do the Materials and Methods page, and looked at different examples. For the rest of the day, since I got back to working in lab at 1, I spent learning about DuckDB and integrating python with SQL. During this, using Gemini's Assistance from two days ago, I refined the code into the first rough draft of the schema extractor. Then using my Gemini Assisstant, I learned about how to use the LLM Environment Variables I created yesterday to turn human prompts into SQL querys that the LLM can run to grab information from the dataframe - essentially allowing the user to "Converse" their data.
### 6/20/2025
Today, I started by working on the software that turns human prompts into SQL querys. After conversing with my Gemini Assistant, I had a fully coded version of a python file that does translates human prompts into SQL queries. I used the environment variables I created earlier in the week to use CyVerse Verde in my program. At the end, I tested it with dummy data, and it seemed to work. My continued task for this is to continue feeding human input and engineering the prompt to get the desired DuckDB SQL query. Then at 1pm, I joined the stand-up meeting with Dr. Swetnam, to whom I told my progress. He gave me suggestions on how to improve the website, which is also now on my to-do list, and told me to continue doing what im doing beacuse I am making good progress. I also asked him for some datasets to test both programs I have. I then started to refine my schema extractor by feeding it a dataset: [Precision Aging Network](https://pandb.org/). However, I was running into many errors, as it was only reading the dummy data, and not the CSV file, even when the CSV file is hardcoded into the program.
### 6/23/2025
Today, I started by writing the 'Materials and Methods' page on my poster, and writing up the two paragraph assigngment explaining the process of my project. You can view it here: [Assignment 3](). Then I started to work on my schema extracton - now making it so that the file paths are not hardcoded, and so that the user can now input their own file. First, to make my workflow more efficient and more clean, I downloaded [Rainbow CSV](https://github.com/mechatroner/rainbow_csv) into VS Code, so that .csv files become easier to read. After using my Gemini Assistant to code the ability for the user to input files in the same directory. I tested it with the precision Aging Network, but it was always running index errors. This was because the first few lines of the file didn't have full sets of data, so I added another user question that asked: at what line in the file does the data start. This eliminated the index out of bounds error and made sure the code was running smoothly.
### 6/24/2025
Since, by now, I had two completed functions: a schema extractor and a function that turns human prompt into SQL. Today, I worked on creating a main.py file where I import all the functions and run the funcitons in the order I want them too. This is essentially the backbone of the project: where all the different files come together. In the main.py file, I put all the user questions; some of them include "Converse with your data: what is your inquiry?:". This customizes all the prompts to the user in context of the finished project. Then in that file, once the user gives us the file and a natural language query, I call the method from the LLM_to_SQL file and then run the output of that(which is an SQL Query) to get the desired information from data. So now, I have a rough path working that can 1] ingest a file 2] take user input 3] use a LLM to take user input and turn it into SQL and 4] return the desired information by running the SQL.
### 6/25/2025
Today, I started brainstomring about the full-stack project, i.e. how I want the user to interact with the software and the different features it can do. I settled on a general structure where the user inputs a .csv file of data and the software will do two things: 1] suggest the different types of visualizations based on the dataset and 2] takes input from the user on what visualizations they want and then plot them. To start creating the full-stack application by using [FastAPI](https://fastapi.tiangolo.com/) to code the backend of the project. I followed Gemini's code and guide and started creating the FastAPI Application in main.py, however I noticed that there was some issues in Gemini's code. Gemini wanted to put new methods in the main file for visualization, but I decided to move it to another file called visualation.py. But before I started coding, I realized that I needed to create a method that Gemini didn't include that was able to read user input from natural language and match it to a supported plot. I used Verde API to create a prompt that does that.
### 6/26/2025
Today, I started by finished the method: recognize_visualization_request, which I changed to return a list of strings so that the final dashboard will have multiple visualizaiton. Although, I limited it to four so that the dashboard doesnt get too cluttered. I also started coding another method called suggest_visualizations which also returns a list of strings of a max length of four, but this one using Verde API to return suggestions simply based on the characterisitics of the data in the csv file. After refining both prompts for recognize_visualization_request and suggest_visualations, I finished both methods and started working on the bulk of the visualization by creating the visualization method itself. I used Gemini to create the plot logic for me for each of the different plots. And I finalized the plots that I want the dashboard to create: "SUPPORTED_PLOT_TYPES = ["BAR_CHART", "LINE_CHART", "SCATTER_PLOT", "HISTOGRAM", "BOX_PLOT", "PIE_CHART"]"

### 6/27/2025
I spent the entirety of today finishing up the visualization file. I started by switching my visualization library to plotly, which is better integrated with Quarto, which is the software I will be using to create the interactive visualizations. I then used Gemini to help me finish all the code for visualization, and decided to test it with AI generated dummy data. However I was running into an error that my API Key wasn't being read, and after some time debugging, I realized it was simply because there was a typo in my API Key environment variable. Next I faced another error which was that the methods were not outputting a readable JSON file, so after some more debugging and help from Gemini, I used the rint(pio.to_json(go.Figure(generated_figures[0]), pretty=True)) function to use plotly to directly change the generated figure to a .JSON file, which worked. The dummy data outputted a bar chart, so I knew my code was working.

### 6/30/2025
Today, I started by cleaning up this GitHub Pages website by backlogging the past four days.
